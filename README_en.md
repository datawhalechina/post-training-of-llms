<div align=center><h1>Post-Training of LLMs</h1></div>

<div align="center">
<a href="README.md">中文</a> | English
</div>

&emsp;&emsp;This project is built around the Post-Training for LLMs course series produced by DeepLearning.AI, providing Chinese translation and knowledge organization tutorials. We offer course content translation, knowledge point organization, and example code, aiming to lower language barriers and help students, researchers, and developers systematically master the core technologies and practical methods of the post-training phase of Large Language Models (LLMs).

&emsp;&emsp;**Online Video Course:** [DeepLearning.AI - Post-training of LLMs](https://www.deeplearning.ai/short-courses/post-training-of-llms/)

&emsp;&emsp;The main content of this project includes: 1. Basic theory and practical applications of Supervised Fine-tuning (SFT), helping learners master how to fine-tune pre-trained models through supervised methods; 2. In-depth analysis of Direct Preference Optimization (DPO) technology, including theoretical foundations and practical operation guidance; 3. Applications of Online Reinforcement Learning (Online RL) in large model post-training, covering the complete process from basic concepts to advanced practices; 4. Complete code examples and practical projects to ensure learners can transform theoretical knowledge into practical application capabilities.

&emsp;&emsp;**The main goal of this project is to enable more students, researchers, and developers to systematically learn and master the core technologies of large language model post-training! Anyone can raise issues or submit PRs to jointly build and maintain this project.**

&emsp;&emsp;Students who want to participate deeply can contact us, and we will add you to the project maintainers.

> &emsp;&emsp;***Learning Suggestions: The learning recommendation for this project is to first learn the basic theory of Supervised Fine-tuning (SFT), then learn Direct Preference Optimization (DPO) technology, and finally delve into the applications of Online Reinforcement Learning (Online RL). Because SFT is the foundation of post-training, DPO is advanced technology, and Online RL is high-level application. Beginners are advised to learn progressively according to the course sequence.***

## Project Structure
#### 1. Table of Contents
- [Chapter 1](./docs/chapter1)
    - [1.1 Course Introduction](./docs/chapter1/chapter1_1)
    - [1.2 Introduction to Post-training Techniques](./docs/chapter1/chapter1_2)
- [Chapter 2](./docs/chapter2)
    - [2.1 Supervised Fine-tuning Theory](./docs/chapter2/chapter2_1)
    - [2.2 Supervised Fine-tuning Practice](./docs/chapter2/chapter2_2)
- [Chapter 3](./docs/chapter3)
    - [3.1 Direct Preference Optimization Theory](./docs/chapter3/chapter3_1)
    - [3.2 Direct Preference Optimization Practice](./docs/chapter3/chapter3_2)
- [Chapter 4](./docs/chapter4)
    - [4.1 Online Reinforcement Learning Theory](./docs/chapter4/chapter4_1)
    - [4.2 Online Reinforcement Learning Practice](./docs/chapter4/chapter4_2/)
- [Chapter 5](./docs/chapter5/)


## Completion Status

| Chapter                                    | Assignee        | Deadline | Status |
| ------------------------------------------ | --------------- | -------- | ------ |
| 1.1 Course Introduction                    | Li Kechen       | 10.7     | ✅     |
| 1.2 Introduction to Post-training          | Li Kechen       | 10.7     | ✅     |
| 2.1 Supervised Fine-tuning Theory         | Zhu Guangen     | 10.7     | ✅     |
| 2.2 Supervised Fine-tuning Practice       | Wang Zeyu       | 10.7     | ✅     |
| 3.1 Direct Preference Optimization Theory | Wang Haihong    | 10.7     | ✅     |
| 3.2 Direct Preference Optimization Practice| Zhang Hongli    | 10.7     | ✅     |
| 4.1 Online Reinforcement Learning Theory  | Zhu Boxiang     | 10.7     | ✅     |
| 4.2 Online Reinforcement Learning Practice| Cai Xuanqi, Zhu Boxiang | 10.7 | ✅ |
| 5.1 Summary                               | Zhang Hongli    | 10.7     | ✅     |


## Acknowledgments

- Special thanks to [@Datawhale](https://github.com/datawhalechina) for supporting this project
- If you have any ideas, please feel free to contact us, and we welcome everyone to raise issues
- Special thanks to the following students who contributed to this tutorial!

<div align=center style="margin-top: 30px;">
  <a href="https://github.com/datawhalechina/Post-training-of-LLMs/graphs/contributors">
    <img src="https://contrib.rocks/image?repo=datawhalechina/Post-training-of-LLMs" />
  </a>
</div>

## Follow Us

<div align=center>
<p>Scan the QR code below to follow our WeChat Official Account: Datawhale</p>
<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/qrcode.jpeg" width = "180" height = "180">
</div>

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=datawhalechina/Post-training-of-LLMs&type=Date)](https://star-history.com/#datawhalechina/Post-training-of-LLMs&Date)

## License

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.

*Note: CC 4.0 license is used by default, but you can choose other licenses based on your project needs*